
.. image:: pystat.png
    :height: 20
    :alt: Statistique
    :target: http://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx/td_2a_notions.html#pour-un-profil-plutot-data-scientist

Compression de modèles de deep learning
+++++++++++++++++++++++++++++++++++++++

Les modèles de deep learning sont très gros.
500 Mo n'est plus aberrant. Si l'apprentissage est réalisé
sur des machines ou des fermes de machines importantes,
il est difficile d'avoir accès à la même puissance pour
la prédiction.

*Lectures*

* `Embedded and mobile deep learning research notes <https://github.com/csarron/emdl>`_
* `A Survey of Model Compression and Acceleration for Deep Neural Networks <https://arxiv.org/abs/1710.09282>`_
* `Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding <https://arxiv.org/abs/1510.00149>`_
* `Learning both Weights and Connections for Efficient Neural Networks <https://arxiv.org/pdf/1506.02626.pdf>`_
* `Efficient Methods and Hardware for Deep Learning <https://purl.stanford.edu/qf934gh3708>`_
* `Compressing Neural Networks with the Hashing Trick <https://arxiv.org/abs/1504.04788>`_
* `Deep Gradient Compression Reducing the Communication Bandwith for Distributed Training <https://arxiv.org/pdf/1712.01887.pdf>`_
* `Compressed Sensing using Generative Models <https://arxiv.org/pdf/1703.03208.pdf>`_
* `Compressing Deep Convolutional Networks using Vector Quantization <https://arxiv.org/abs/1412.6115>`_
* `ThiNet: AFilterLevelPruningMethodforDeepNeuralNetworkCompression <https://arxiv.org/pdf/1707.06342.pdf>`_

*Modèles*

* `référence de modèles compressés <https://github.com/csarron/emdl#model>`_

*Code*

* `Neural Network Pruning PyTorch Implementation <https://github.com/wanglouis49/pytorch-weights_pruning>`_

*Modules*

* `webdnn <https://github.com/mil-tokyo/webdnn>`_
* `ncnn <https://github.com/Tencent/ncnn>`_
* `ELL <https://github.com/Microsoft/ELL>`_
